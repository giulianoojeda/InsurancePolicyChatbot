{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re5RjlGMjo0e"
   },
   "source": [
    "# Exploratory Data Analysis - Insurance Policy Generation Chatbot\n",
    "\n",
    "Exploratory Data Analysis (EDA) for textual data from PDFs is a multi-step process that involves:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Initialization:\n",
    "\n",
    "Let's Import dependencies and initialize some variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDxhsbcHjoTg",
    "outputId": "56f79297-58b8-4868-df66-b04fea5dc2d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import openai api and set api key\n",
    "import openai\n",
    "\n",
    "openai.api_key = config.OPENAI_API_KEY\n",
    "\n",
    "# import src modules\n",
    "from src import config\n",
    "from src import ETL\n",
    "\n",
    "# import langchain related modules\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KanRjZ-ke3O"
   },
   "source": [
    "## Data Extraction:\n",
    "\n",
    "In this section, we will download the data from an s3 bucket and save it locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "oRtLZyGziNpn",
    "outputId": "9639891f-6247-41b1-8e72-c70a4d618d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files from anyoneai-datasets/queplan_insurance/ already downloaded to /home/gio/ANYONEAI/InsurancePolicyChatbot/dataset.\n"
     ]
    }
   ],
   "source": [
    "ETL.extract_from_s3(\n",
    "    config.S3_BUCKET_NAME,\n",
    "    config.S3_BUCKET_PREFIX,\n",
    "    config.AWS_ACCESS_KEY_ID,\n",
    "    config.AWS_SECRET_ACCESS_KEY,\n",
    "    config.DATASET_ROOT_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loading:\n",
    "\n",
    "Now let's load the PDFs containing Insurance Policies using PyPDFDirectoryLoader from LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(config.DATASET_ROOT_PATH)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Page is a `Document`.\n",
    "\n",
    "A `Document` contains text (`page_content`) and `metadata`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 267\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of pages: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at it's metadata. We can see that it contains the following fields:\n",
    "\n",
    "- `source` : The source of the document\n",
    "- `page` : The page number of the document\n",
    "\n",
    "this information can be used to trace back the document to the source for debugging purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Page's File name is:  /home/gio/ANYONEAI/InsurancePolicyChatbot/dataset/POL320130223.pdf\n",
      "The Page number is:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"The Page's File name is: \", documents[0].metadata[\"source\"])\n",
    "\n",
    "print(\"The Page number is: \", documents[0].metadata[\"page\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at a glimpse of the first page of the first document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGURO COLECTIVO COMPLEMENTARIO DE SALUD \n",
      "Incorporada al Depósito de Pólizas bajo el código POL320130223\n",
      "ARTICULO 1°: REGLAS APLICABLES AL CONTRATO\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Se aplicarán al presente contrato de seguro las disposiciones contenidas en los artículos siguientes y las\n",
      "normas legales de carácter imperativo establecidas en el Título VIII, del Libro II, del Código de Comercio. Sin\n",
      "embargo, se entenderán válidas las estipulaciones contractuales que sean más beneficiosas para el\n",
      "asegurado o beneficia ...\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content[0:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we explore the first page of the first document is to get a sense of the structure of the document. This analisis will help us in the next step which is the Document Splitting phase, where we will split the document into chunks to feed our vector store. In order to ensure that each chunk has coherent, self-contained information, we need to define de appropriate chunk size.\n",
    "\n",
    "Given the content of our dataset, the logical division would be to break down by articles and major subheadings since they seem to encapsulate a singular topic or concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:\n",
    "\n",
    "However, we see the presence of a lot of whitespace and empty lines across the dataset. This might interfere with the chunking process and produce poor results. Therefore, we will preprocess the data to remove the empty lines and whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will clean the pages from extra whitespaces and newlines\n",
    "ETL.preprocess(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "E) BENEFICIO DE SALUD MENTAL\n",
      "F) BENEFICIOS ESPECIALES\n",
      "Para los efectos de tener derecho a los beneficios que emanan de estas coberturas, éstas deberán estar\n",
      "expresamente señaladas en las Condiciones Particulares de la póliza, en las cuales se establecerán además\n",
      "los porcentajes y límites de reembolso o pago correspondientes a cada una de ellas.\n",
      "A) BENEFICIO DE HOSPITALIZACIÓN:\n",
      "Bajo este beneficio se cubren los gastos médicos incurridos en complemento de lo que cubra el sistema de\n",
      "salud previsional o de bienestar u otro seguro o convenio, de acuerdo a los porcentajes y límites de\n",
      "reembolso o pago definidos para este beneficio en el Cuadro de Beneficios de las Condiciones Particulares\n",
      "de la póliza.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents[3].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Splitting\n",
    "\n",
    "Now, let's split the document into chunks of text for further processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Considerations\n",
    "\n",
    "Several variables play a role in determining the best chunking strategy, and these variables vary depending on the use case. Here are some key aspects to keep in mind:\n",
    "\n",
    "1. **What is the nature of the content being indexed?**\n",
    "\n",
    "   Based on the content of the dataset, we're working with insurance policies. The logical division would be to break down by articles and major subheadings since they seem to encapsulate a singular topic or concept.\n",
    "\n",
    "2. **Which embedding model will be used, and what chunk sizes does it perform optimally on?**\n",
    "\n",
    "   We will be using OpenAI GPT-3.5 Turbo, which performs optimally on chunks of 512 tokens.\n",
    "\n",
    "3. **What are your expectations for the length and complexity of user queries?**\n",
    "\n",
    "   Since this solution will act as a chatbot, we can expect the queries to be mostly short and simple. However, we should also consider the possibility of more complex queries, such as \"Cual es la diferencia entre el seguro de vida y el seguro de salud?\" (What is the difference between life insurance and health insurance?)\n",
    "\n",
    "4. **How will the retrieved results be utilized within your specific application?**\n",
    "\n",
    "   The retrieved results will be used to answer user queries. The user will be able to ask questions about the content of the documents, and the chatbot will respond with the most relevant information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking methods\n",
    "\n",
    "There are different methods for chunking, and each of them might be appropriate for different situations. By examining the strengths and weaknesses of each method, our goal is to identify the right scenario to apply them to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed-size chunking\n",
    "\n",
    "This is the most common and straightforward approach to chunking: we simply decide the number of tokens in our chunk and, optionally, whether there should be any overlap between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size count:  267\n",
      "SEGURO COLECTIVO COMPLEMENTARIO DE SALUD \n",
      "Incorporada al Depósito de Pólizas bajo el código POL320130223\n",
      "ARTICULO 1°: REGLAS APLICABLES AL CONTRATO\n",
      "Se aplicarán al presente contrato de seguro las disposiciones contenidas en los artículos siguientes y las\n",
      "normas legales de carácter imperativo establecidas en el Título VIII, del Libro II, del Código de Comercio. Sin\n",
      "embargo, se entenderán válidas las estipulaciones contractuales que sean más beneficiosas para el\n",
      "asegurado o beneficiario.\n",
      "ARTÍCULO Nº 2: COBERTURA\n",
      "La compañía de seguros bajo las condiciones y términos que más adelante se establecen, conviene en\n",
      "reembolsar o pagar al beneficiario, los gastos médicos razonables y acostumbrados en que haya incurrido\n",
      "efectivamente un asegurado, en complemento de lo que cubra el sistema de salud previsional o de bienestar\n",
      "u otro seguro o convenio, a consecuencia de una incapacidad cubierta. ...\n"
     ]
    }
   ],
   "source": [
    "c_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "c_splitter_result = c_splitter.split_documents(documents)\n",
    "\n",
    "print(\"Chunk Size count: \", len(c_splitter_result))\n",
    "\n",
    "print(c_splitter_result[0].page_content, \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Chunking\n",
    "\n",
    "Recursive chunking divides the input text into smaller chunks in a hierarchical and iterative manner using a set of separators. If the initial attempt at splitting the text doesn’t produce chunks of the desired size or structure, the method recursively calls itself on the resulting chunks with a different separator or criterion until the desired chunk size or structure is achieved. This means that while the chunks aren’t going to be exactly the same size, they’ll still “aspire” to be of a similar size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size count:  734\n",
      "SEGURO COLECTIVO COMPLEMENTARIO DE SALUD \n",
      "Incorporada al Depósito de Pólizas bajo el código POL320130223\n",
      "ARTICULO 1°: REGLAS APLICABLES AL CONTRATO\n",
      "Se aplicarán al presente contrato de seguro las disposiciones contenidas en los artículos siguientes y las\n",
      "normas legales de carácter imperativo establecidas en el Título VIII, del Libro II, del Código de Comercio. Sin\n",
      "embargo, se entenderán válidas las estipulaciones contractuales que sean más beneficiosas para el\n",
      "asegurado o beneficiario.\n",
      "ARTÍCULO Nº 2: COBERTURA\n",
      "La compañía de seguros bajo las condiciones y términos que más adelante se establecen, conviene en\n",
      "reembolsar o pagar al beneficiario, los gastos médicos razonables y acostumbrados en que haya incurrido\n",
      "efectivamente un asegurado, en complemento de lo que cubra el sistema de salud previsional o de bienestar\n",
      "u otro seguro o convenio, a consecuencia de una incapacidad cubierta. ...\n"
     ]
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    strip_whitespace=True,\n",
    ")\n",
    "\n",
    "r_splitter_result = r_splitter.split_documents(documents)\n",
    "\n",
    "print(\"Chunk Size count: \", len(r_splitter_result))\n",
    "\n",
    "print(r_splitter_result[0].page_content, \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage\n",
    "\n",
    "Now that we have our chunks, we can feed them to our vector store. But first, we need to convert them into embeddings.\n",
    "\n",
    "### Embedding\n",
    "\n",
    "We will use the OpenAI GPT-3.5 Turbo model to generate embeddings for our chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store\n",
    "\n",
    "We will use Chroma to store our embeddings. Chroma is a vector store that allows us to store and query embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-NZFzrEuZsJ8hLl82wd85Ec4b on tokens per min. Limit: 150000 / min. Current: 0 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-NZFzrEuZsJ8hLl82wd85Ec4b on tokens per min. Limit: 150000 / min. Current: 1 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-NZFzrEuZsJ8hLl82wd85Ec4b on tokens per min. Limit: 150000 / min. Current: 0 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-NZFzrEuZsJ8hLl82wd85Ec4b on tokens per min. Limit: 150000 / min. Current: 1 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 10.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-NZFzrEuZsJ8hLl82wd85Ec4b on tokens per min. Limit: 150000 / min. Current: 1 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gio/ANYONEAI/InsurancePolicyChatbot/EDA.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gio/ANYONEAI/InsurancePolicyChatbot/EDA.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m vector_store \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gio/ANYONEAI/InsurancePolicyChatbot/EDA.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     documents, embedding, persist_directory\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mCHROMA_PATH\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gio/ANYONEAI/InsurancePolicyChatbot/EDA.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m )\n",
      "File \u001b[0;32m~/ANYONEAI/InsurancePolicyChatbot/.venv/lib/python3.11/site-packages/langchain/vectorstores/chroma.py:646\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m texts \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    645\u001b[0m metadatas \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 646\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(\n\u001b[1;32m    647\u001b[0m     texts\u001b[39m=\u001b[39;49mtexts,\n\u001b[1;32m    648\u001b[0m     embedding\u001b[39m=\u001b[39;49membedding,\n\u001b[1;32m    649\u001b[0m     metadatas\u001b[39m=\u001b[39;49mmetadatas,\n\u001b[1;32m    650\u001b[0m     ids\u001b[39m=\u001b[39;49mids,\n\u001b[1;32m    651\u001b[0m     collection_name\u001b[39m=\u001b[39;49mcollection_name,\n\u001b[1;32m    652\u001b[0m     persist_directory\u001b[39m=\u001b[39;49mpersist_directory,\n\u001b[1;32m    653\u001b[0m     client_settings\u001b[39m=\u001b[39;49mclient_settings,\n\u001b[1;32m    654\u001b[0m     client\u001b[39m=\u001b[39;49mclient,\n\u001b[1;32m    655\u001b[0m     collection_metadata\u001b[39m=\u001b[39;49mcollection_metadata,\n\u001b[1;32m    656\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    657\u001b[0m )\n",
      "File \u001b[0;32m~/ANYONEAI/InsurancePolicyChatbot/.venv/lib/python3.11/site-packages/langchain/vectorstores/chroma.py:610\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \n\u001b[1;32m    584\u001b[0m \u001b[39mIf a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[39m    Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m chroma_collection \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[1;32m    602\u001b[0m     collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[1;32m    603\u001b[0m     embedding_function\u001b[39m=\u001b[39membedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    609\u001b[0m )\n\u001b[0;32m--> 610\u001b[0m chroma_collection\u001b[39m.\u001b[39;49madd_texts(texts\u001b[39m=\u001b[39;49mtexts, metadatas\u001b[39m=\u001b[39;49mmetadatas, ids\u001b[39m=\u001b[39;49mids)\n\u001b[1;32m    611\u001b[0m \u001b[39mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[0;32m~/ANYONEAI/InsurancePolicyChatbot/.venv/lib/python3.11/site-packages/langchain/vectorstores/chroma.py:188\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m texts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(texts)\n\u001b[1;32m    187\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_documents(texts)\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m metadatas:\n\u001b[1;32m    190\u001b[0m     \u001b[39m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[39m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     length_diff \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(texts) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/ANYONEAI/InsurancePolicyChatbot/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:483\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \n\u001b[1;32m    473\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[0;32m~/ANYONEAI/InsurancePolicyChatbot/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:367\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    364\u001b[0m     _iter \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(tokens), _chunk_size)\n\u001b[1;32m    366\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m _iter:\n\u001b[0;32m--> 367\u001b[0m     response \u001b[39m=\u001b[39m embed_with_retry(\n\u001b[1;32m    368\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    369\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens[i : i \u001b[39m+\u001b[39;49m _chunk_size],\n\u001b[1;32m    370\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invocation_params,\n\u001b[1;32m    371\u001b[0m     )\n\u001b[1;32m    372\u001b[0m     batched_embeddings\u001b[39m.\u001b[39mextend(r[\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    374\u001b[0m results: List[List[List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(texts))]\n",
      "File \u001b[0;32m~/ANYONEAI/InsurancePolicyChatbot/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:107\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     response \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m _check_response(response, skip_empty\u001b[39m=\u001b[39membeddings\u001b[39m.\u001b[39mskip_empty)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m _embed_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ANYONEAI/InsurancePolicyChatbot/.venv/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/ANYONEAI/InsurancePolicyChatbot/.venv/lib/python3.11/site-packages/tenacity/__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    388\u001b[0m     retry_state\u001b[39m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msleep(do)\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m~/ANYONEAI/InsurancePolicyChatbot/.venv/lib/python3.11/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msleep\u001b[39m(seconds: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     time\u001b[39m.\u001b[39msleep(seconds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "    documents, embedding, persist_directory=config.CHROMA_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(vector_store.count())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
